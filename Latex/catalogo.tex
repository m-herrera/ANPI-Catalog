\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,multicol,enumerate}
\usepackage{amsfonts}
\usepackage{multicol}

\usepackage{wrapfig}

\usepackage{listings}
\usepackage{xcolor}
 
\usepackage[spanish,onelanguage]{algorithm2e}

 
%%%%% Formato REVISTA DE MATEMÁTICA: TEORÍA Y APLICACIONES%%%%%%%
\topmargin=-2cm\textheight=23cm\textwidth=19cm
\oddsidemargin=-1cm\evensidemargin=-1cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\parskip=0.25cm
\parindent=0mm

\renewcommand{\figurename}{Figura}

%\usepackage[dvips]{graphicx}
\usepackage{url}
\usepackage{float}
%Utilizamos el paquete para incorporar gráficos postcript


%\usepackage{amssymb}
\usepackage[psamsfonts]{amssymb} %paquetes para los símbolos matemáticos
\usepackage{latexsym}


\usepackage{colortbl}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{backgroundStyle}{
  backgroundcolor=\color{backcolour},
  captionpos=b,
}

\lstset{style=backgroundStyle}

\begin{document}


%Encabezado
Instituto Tecnológico de Costa Rica \hfill CE-3102: Análisis Numéricos para Ingeniería\\
Ingeniería en Computadores \hfill Semestre: II - 2019\\
Nombre: Marco Herrera Valverde \hfill Carné: 2017111230


\begin{center}\textbf{\huge Catálogo del Curso}  \end{center}


\section{Solución de Ecuaciones No Lineales}

\subsection{Método de la Bisección}

\subsubsection{Fórmula Matemática}
\[ I_{k+1} = [a_{k+1},b_{k+1}] = \begin{cases} 
      [a_k,x_k] & \text{si } f(a_k)f(x_k) < 0 \\
      [x_k,b_k] & \text{si } f(a_k)f(x_k) > 0 
   \end{cases}\\
   \; \text{ donde } x_k = \frac{a+b}{2} 
\]
\subsubsection{Descripción breve del método}
Se parte de una función \(f\) continua en un intervalo \([a,b]\), en el cual se cumple el teorema de balzano (\(f(a)f(b) < 0\)), este intervalo es posteriormente dividido en 2 subintervalos de igual tamaño \([a,x] [x,b]\), siendo \(x\) el punto medio del intervalo, luego se aplica el teorema de balzano a ambos subintervalos y se repite el procedimiento con aquél que lo cumpla, hasta alcanzar una tolerancia deseada. El método genera una sucesión \(\{x_{k}\}_{k=1}^\infty\) que converge a \(\xi \in [a,b]\) tal que \(f(\xi) = 0\).

\begin{itemize}
    \item Ventajas: intuitivo, fácil de implementar, siempre converge.
    \item Desventajas: converge lentamente, requiere conocer el intervalo inicial.
    \item Convergencia: Lineal
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método Bisección}
\SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Extremos del rango inicial, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
     \SetAlgoLined
    \If{\( f(a)f(b)<0\)}{

    \While{f(aprox) \(\;\geq\;\)tolerancia}{
    \(aprox = \frac{a+b}{2}\)\;
      \eIf{ \(f(a)f(aprox)\;<\;0\)}{b=aprox}{a=aprox}
      aumentar conteo de iteraciones\;
     }
    }
     
\end{algorithm}
\subsubsection{Código OCTAVE del Método}

%Python code highlighting
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de bisección en Octave]
## Metodo iterativo de biseccion para la solucion de ecuaciones no lineales.
## Entradas: rango inicial, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x_k, nIterations] = biseccion (a, b, tolerance, functionStr)
  func = str2func(functionStr);
  x_k = (a + b) / 2;  
  nIterations = 0;
  if (func(a) * func(b) < 0)
    while abs(func(x_k)) >= tolerance
      if func(a) * func(x_k) < 0
        b = x_k;
      else
        a = x_k;
      end
      nIterations += 1;
      x_k = (a + b) / 2;
    endwhile
  endif
endfunction
\end{lstlisting}
\subsubsection{Código Python del Método}

%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de bisección en python]
import math

"""
Metodo iterativo de biseccion, para solucion de ecuaciones no lineales.
Entradas: limites del rango inicial de iteracion, tolerancia minima del
resultado y funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def biseccion(a, b, tolerance, function):
    if eval(function)(a) * eval(function)(b) > 0:
        return 0, 0
    nIterations = 0
    while True:
        x_k = (a + b) / 2
        if abs(eval(function)(x_k)) < tolerance:
            break
        elif eval(function)(a) * eval(function)(x_k) < 0:
            b = x_k
        else:
            a = x_k
        nIterations += 1
    return x_k, nIterations
    \end{lstlisting}


\subsection{Método de Newton-Raphson}

\subsubsection{Fórmula Matemática}
\[\begin{cases} 
      x_{k+1}= x_k -\frac{f(x_k)}{f'(x_k)}\\
      x_0 \in \mathbb{R}
   \end{cases}\\
   \text{donde}\; f'(x_k) \neq 0, \forall \;k \geq 0
   \]

\subsubsection{Descripción breve del método}
Surge de la expansión de taylor truncada en el segundo término para una función dada.


Sea \(f\) una función continua en \([a,b]\) y dos veces diferenciable, donde cada derivada es continua en \([a,b]\). Si \(\xi; \in; [a,b].\) es tal que \(f(\xi)=0;y;(f'(\xi)\neq0\), entonces existe \(\delta >0\) tal que el método de Newton-Raphson guenera una sucesión \({x_k}_{k=1}^\infty\) que converge a \(\xi\), para cualquier aproximación inicial \(x_0 \in [\xi-\delta,\xi+\delta]\).

\begin{itemize}
    \item Ventajas: cuando converge, es uno de los más rápidos en converger, a un orden cuadrático.
    \item Desventajas: no converge si la recta tangente en un punto es cero, usualmente solo se espera que converja alrededor de la solución, es caro pues necesita evaluar la función y la derivada. 
    \item Convergencia: cuadrática para una solución, lineal para múltiples soluciones.
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método Newton-Raphson}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valor inicial, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
     \SetAlgoLined

     \While{\(f(aprox \geq tolerancia\)}{
      \(aprox = aprox - \frac{f(aprox)}{f'(aprox)}\)\;
      aumentar conteo de iteraciones\;
     }
\end{algorithm}

\subsubsection{Código OCTAVE del Método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de bisección en Octave]
## Metodo iterativo de Newton-Raphson para la solucion de ecuaciones
## no lineales.
## Entradas: rango inicial, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x_k, nIterations] = newtonRaphson (x0, tolerance, functionStr)
  syms x;
  func = str2func(functionStr);
  deriv = diff(func, x);
  nIterations = 0;
  x_k = x0;
  while abs(func(x_k)) >= tolerance
    x_k = x_k - func(x_k) / eval((subs(deriv, x, x_k)));
    nIterations += 1;
  endwhile
endfunction
\end{lstlisting}

\subsubsection{Código Python del Método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de Newton-Raphson en python]
import math
import sympy

"""
Metodo iterativo de Newton-Raphson, para solucion de ecuaciones no lineales.
Entradas: valor inicial de iteracion, tolerancia minima del resultado y 
funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def newton_raphson(x0, tolerance, function):
    x = sympy.symbols('x')
    nIterations = 0
    x_k = x0
    while abs(eval(function)(x_k))  >= tolerance:
        deriv = sympy.diff(eval(function)(x), x)
        x_k = x_k - eval(function)(x_k) / deriv.evalf(subs={x: x_k})
        nIterations += 1
    return x_k, nIterations
    \end{lstlisting}


\subsection{Método de la Secante}

\subsubsection{Fórmula Matemática}
\[\begin{cases} 
      x_{k+1}= x_k -\frac{f(x_k)(x_k-x_{k-1})}{f(x_k)-f(x_{k-1})}\\
      x_0, x_1 \in \mathbb{R}
   \end{cases}\\
   \text{donde}\; f(x_k) - f(x_{k-1}) \neq 0, \forall \;k \geq 0
   \]

\subsubsection{Descripción breve del método}
Surge a partir del método de Newton-Raphson, busca eliminar la necesidad de calcular la derivada, mediante la definición de esta; por lo que requiere un valor inicial adicional. Traza una recta por dos puntos de la curva de la función y define un tercero como el punto donde la recta anterior interseca al eje de las abscisas. De esta forma define dos puntos nuevos y se repite el procedimiento, hasta encontrar un valor aceptable. Al igual que los métodos anteriores, genera una sucesión de términos que convergen a una raíz de la función.

\begin{itemize}
    \item Ventajas: Se puede aplicar en funciones muy complejas donde la derivada es difícil de obtener.
    \item Desventajas: Velocidad de convergencia menor a la del método Newton-Raphson, la convergencia no se asegura si los valores iniciales son muy lejanos la valor de convergencia. Requiere de dos valores iniciales.
    \item Convergencia: Superlineal
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de la secante}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
     \SetAlgoLined

     \While{\(f(aprox \geq tolerancia\)}{
     temp = aprox\;
      \(aprox = aprox - \frac{f(aprox)(aprox - aprox\_anterior)}{f(aprox)-f(aprox\_anterior)}\)\;
      aprox\_anterior = temp\;
      aumentar conteo de iteraciones\;
     }
\end{algorithm}

\subsubsection{Código OCTAVE del Método}

\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de la secante en Octave]
## Metodo iterativo de la secante para la solucion de ecuaciones no lineales.
## Entradas: rango inicial, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x_k, nIterations] = secante(x0, x1, tolerance, functionStr)
  func = str2func(functionStr);
  x_k = x1;
  x_k_1 = x0;
  nIterations = 0;
  while abs(func(x_k)) >= tolerance
    temp = x_k;
    numerator = func(x_k) * (x_k - x_k_1);
    denominator = func(x_k) - func(x_k_1);
    x_k = x_k - numerator / denominator;
    x_k_1 = temp;
    nIterations += 1;
  endwhile
endfunction
\end{lstlisting}

\subsubsection{Código Python del Método}

%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de la secante en python]
import math

"""
Metodo iterativo de la secante, para solucion de ecuaciones no lineales.
Entradas: valores iniciales de iteracion, tolerancia minima del resultado y 
funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def secante(x0, x1, tolerance, function):
    x_k = x1
    x_k_1 = x0
    nIterations = 0
    while abs(eval(function)(x_k))  >= tolerance:
        temp = x_k
        numerator = (eval(function)(x_k) * (x_k - x_k_1))
        denominator = (eval(function)(x_k) - eval(function)(x_k_1))
        x_k = x_k - numerator / denominator
        x_k_1 = temp
        nIterations += 1
    return x_k, nIterations
    \end{lstlisting}

\subsection{Método de la posición falsa}

\subsubsection{Fórmula Matemática}
\[ I_{k+1} = [a_{k+1},b_{k+1}] = \begin{cases} 
      [a_k,x_k] & \text{si } f(a_k)f(x_k) < 0 \\
      [x_k,b_k] & \text{si } f(a_k)f(x_k) > 0 
   \end{cases}\\
   \;
\]
\[ x_{k+1}= x_k -\frac{f(x_k)(x_k-c_k)}{f(x_k)-f(c_k)} \text{ }\; \text{donde}\;
c_k = \begin{cases} 
      a_k & \text{si } f(a_k)f(x_k) < 0 \\
      b_k & \text{si } f(a_k)f(x_k) > 0
   \end{cases}\\
\]
\[ \text{Con } x_0 = a\text{ y } x_1 = b\]

\subsubsection{Descripción breve del método}
Surge a partir de la unión del método de bisección y el método de la secante, aplica el principio de producir sub-intervalos que buscan aproximar la solución, pero a diferencia del método de bisección, la división del intervalo no se hace mediante el punto medio, sino se aplica el principio del método de la secante, donde el siguiente punto se obtiene a partir de la recta entre los extremos del intervalo, en su intersección con el eje de las abscisas; este punto permite escoger un nuevo intervalo más pequeño mediante el teorema de balzano.

\begin{itemize}
    \item Ventajas: Es siempre convergente para funciones continuas.
    \item Desventajas: Aunque converge más rápido que el método de bisección, su velocidad de convergencia es baja.
    \item Convergencia: Lineal
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de la posición falsa}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
    \SetAlgoLined
    \If{\(f(a)f(b) < 0\)}{
        aprox = b\;
        \While{ \(\|f(aprox)\| \geq tol\)}{
            \(aprox = b - \frac{f(b)(b-a)}{f(b)-f(a)}\)\;
            \eIf{\(f(a)f(aprox) < 0\)}{
            b = aprox\;}{
            a = aprox\;}
            aumentar conteo de iteraciones\;
            
        }
    }

\end{algorithm}

\subsubsection{Código OCTAVE del Método}

\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de la posición falsa en Octave]
## Metodo iterativo de la posicion falsa para la solucion de ecuaciones
## no lineales.
## Entradas: rango inicial, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x_k, nIterations] = falsaPosicion (a, b, tolerance, functionStr)
  func = str2func(functionStr);
  nIterations = 0;
  x_k = b - func(b) * (b-a) / (func(b) - func(a));
  if func(a)*func(b) <= 0
    while abs(func(x_k)) >= tolerance
      if func(a)*func(x_k) < 0
        b = x_k;
      else
        a = x_k;
      end
      x_k = b - func(b) * (b-a) / (func(b) - func(a));
      nIterations += 1;
    endwhile
  endif
endfunction

\end{lstlisting}

\subsubsection{Código Python del Método}

%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de la posición falsa en Python]
import math

""" 
Metodo iterativo de falsa posicion, para solucion de ecuaciones no lineales.
Entradas: limites de rango incial de iteracion, tolerancia minima del 
resultado y funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def falsa_posicion(a, b, tolerance, function):
    if eval(function)(a) * eval(function)(b) > 0:
        return 0, 0
    nIterations = 0
    while True:
        denominator = (eval(function)(b) - eval(function)(a))
        x_k = b - eval(function)(b) * (b - a) / denominator
        if abs(eval(function)(x_k)) < tolerance:
            break
        elif eval(function)(a) * eval(function)(x_k) < 0:
            b = x_k
        else:
            a = x_k
        nIterations += 1
    return x_k, nIterations
\end{lstlisting}

\subsection{Método del punto fijo}

\subsubsection{Fórmula Matemática}
Existencia del punto fijo.
\[\varphi(x) \in [a,b],\; \forall x\in [a,b]\]
Unicidad del punto fijo.
\[\exists\; L < 1, \text{ tal que } \|\varphi'(x)\|\leq L, \; \forall x \in ]a,b[\]
Método iterativo del punto fijo.
\[ x_{K+1}= \varphi(x_k)\]

\subsubsection{Descripción breve del método}
Un punto fijo es un par ordenado de la función en el que \( f(x) = x\), de forma que la expresión \(f(x) = 0\) se puede reescribir de la forma \(x- \varphi(x) = 0\) donde \(\varphi(x)\) es una función que converge al punto fijo de la función \(f\). Por lo que al encontrar este punto, es equivalente a encontrar la solución de la función original. Sin embargo es necesario garantizar la convergencia de \(\varphi(x)\) al punto fijo, por lo que es necesario primero demostrar su existencia y luego su unicidad.

\begin{itemize}
    \item Ventajas: Implementación muy sencilla, posee condiciones de convergencia. 
    \item Desventajas: Necesidad construir funciones para iterar y deben demostrarse existencia y unicidad.
    \item Convergencia: Lineal.
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método del punto fijo}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, tolerancia mínima y función del punto fijo\;}
    \Output{Aproximación de la solución y número de iteraciones}
     \SetAlgoLined
    \While{\(\|\varphi(x_k)-x_k\| \geq tol\)}{
     \(x_k = \varphi(x_k)\)\;
     aumentar conteo de iteraciones\;
     }
\end{algorithm}

\subsubsection{Código OCTAVE del Método}

\begin{lstlisting}[language=OCTAVE, caption=Implementación del método del punto fijo en Octave]
## Metodo iterativo del punto fijo para la solucion de ecuaciones no lineales.
## Entradas: rango inicial, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x_k, nIterations] = punto_fijo (x0, tolerance, phiStr)
  phi = str2func(phiStr);
  x_k = x0;
  nIterations = 0;
  while abs(phi(x_k) - x_k) >= tolerance
    x_k = phi(x_k);
    nIterations += 1;
  endwhile
endfunction
\end{lstlisting}

\subsubsection{Código Python del Método}

%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método del punto fijo en Python]
import math

"""
Metodo iterativo del punto fijo, para solucion de ecuaciones no lineales.
Entradas: valor incial de iteracion, tolerancia minima del resultado y
funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def punto_fijo(x0, tolerance, phi):
    variable = Symbol('x')
    phi = sympify(phi)
    f = lambdify(variable, phi, "numpy")
    nIterations = 0;
    x_k = x0;
    while abs(f(x_k) - x_k) >= tolerance:
        x_k = f(x_k)
        nIterations += 1
    return x_k, nIterations
\end{lstlisting}

Para el método se anterior, se asume que la función del punto fijo es válida. Se asume su existencia y unicidad.


\subsection{Método de Müller}

\subsubsection{Fórmula Matemática}

\[r = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \]
\[ \left( \begin{array}{ccc}
(x_0)^2 & x_0 & 1\\
(x_1)^2 & x_1 & 1\\
(x_2)^2 & x_2 & 1\\
\end{array} \right)
%
\left( \begin{array}{c}
a \\
b \\
c
\end{array} \right)
=
\left( \begin{array}{c}
f(x_0) \\
f(x_1) \\
f(x_2)
\end{array} \right)
\]




\subsubsection{Descripción breve del método}
El método de Müller se considera una variación del método de la secante, pero a diferencia del anterior; este hace uso de una función cuadrática para el cálculo de la siguiente aproximación. Lo cual significa una mayor velocidad de convergencia; sin embargo, el método requiere de 3 valores iniciales para su iteración. A partir de estos, se calcula una función cuadrática que pase por estos 3 puntos, y se calcula su intersección con el eje de las abscisas, y este punto junto con los dos más cercanos a este, generan la nueva tripleta para iteración.

\begin{itemize}
    \item Ventajas: Permite encontrar soluciones complejas, alta convergencia
    \item Desventajas: Alta dificultad de desarrollo manual, mayor complejidad que el método de secante y no siempre es mejor. Requiere de tres valores iniciales.
    \item Convergencia: Aproximadamente 1.84 
\end{itemize}

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de Müller}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
    \SetAlgoLined

    \While{f(x0) \(\;\geq\;\)tolerancia}{
        resolver para la matriz\;
        \([[x0**2, x0, 1], [x1**2, x1, 1], [x2**2, x2, 1]]\)
        con \([f(x0), f(x1), f(x2)]\)\;
        
        resolver la cuadrática con el resultado de la matriz\;
        
        \(r = (-b \pm math.sqrt(b**2 - 4 * a * c)) / (2 * a)\)\;
        
        Calcular 2 puntos más cercanos para ambos resultados\;
        
        
      \eIf{ \(distancia(r1)\;<\;distancia(r2)\)}{x0 = r1}{x0 = r2}
      asignar puntos más cercanos a x2 y x2\;
      aumentar conteo de iteraciones\;
     }


\end{algorithm}

\subsubsection{Código OCTAVE del Método}

\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de Müller en Octave]
## Metodo iterativo de Muller para la solucion de ecuaciones no lineales.
## Entradas: valores iniciales, tolerancia minima y funcion a evaluar.
## Salidas: aproximacion de la solucion y numero de iteraciones.
function [x0, iteracion] = muller(x0, x1, x2, tol, funcion)
    f = str2func(funcion);
    iteracion = 0;

    function [closest] = get_closest(x0, x1, x2, r)
      closest = [x0 x1 r];
      if abs(x2 - r) < abs(x0 - r)
          closest(1) = x2;
          if abs(x0 - r) < abs(x1 - r)
              closest(2) = x0;
          end
      elseif abs(x2 - r) < abs(x1 - r)
          closest(2) = x2;
      end
    endfunction
    
    function [parameters] = values(a, b, c, x0, x1, x2)
        r1 = (-b + sqrt(b**2 - 4 * a * c)) / (2 * a);
        r2 = (-b - sqrt(b**2 - 4 * a * c)) / (2 * a);
        mins1 = get_closest(x0, x1, x2, r1);
        mins2 = get_closest(x0, x1, x2, r2);
        if (abs(mins1(1) - r1) + abs(mins1(2) - r1) <
            abs(mins2(1) - r2) + abs(mins2(2) - r2))
            parameters = mins1;
        else
            parameters = mins2;
        end
    endfunction
    
    while abs(f(x0)) >= tol
        A = [[x0**2 x0 1]; [x1**2 x1 1]; [x2**2 x2 1]];
        B = [f(x0); f(x1); f(x2)];
        parametros = A \ B;
        a = parametros(1);
        b = parametros(2);
        c = parametros(3);
        next_iter = values(a, b, c, x0, x1, x2);
        x0 = next_iter(1);
        x1 = next_iter(2);
        x2 = next_iter(3);
        iteracion += 1;
    endwhile
endfunction

\end{lstlisting}

\subsubsection{Código Python del Método}

%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de Müller en Python] 
from sympy import *
import math
import numpy as np


"""
Metodo iterativo de muller, para solucion de ecuaciones no lineales.
Entradas: limites del rango inicial de iteracion, tolerancia minima del
resultado y funcion a evaluar en formato lambda.
Salidas:Valor aproximado y numero de iteraciones
"""
def muller(x0, x1, x2, tol, funcion):
    variable = Symbol('x')
    funcion = sympify(funcion)
    f = lambdify(variable, funcion, "numpy")
    iteracion = 0

    while abs(f(x0)) >= tol:
        A = np.array([[x0**2, x0, 1], [x1**2, x1, 1], [x2**2, x2, 1]])
        B = np.array([f(x0), f(x1), f(x2)])
        parametros = np.linalg.solve(A, B)
        a = parametros[0]
        b = parametros[1]
        c = parametros[2]
        next_iter = values(a, b, c, x0, x1, x2);
        x0 = next_iter[0]
        x1 = next_iter[1]
        x2 = next_iter[2]
        iteracion += 1
    return x0, iteracion


"""
Metodo auxiliar de muller, para calculo de soluciones cuadraticas y determinar
solucion apropiada
Entradas: coeficientes de la ecuacion cuadratica, valores de iteracion
Salidas: tupla de proxima iteracion
"""
def values(a, b, c, x0, x1, x2):
    r1 = (-b + math.sqrt(b**2 - 4 * a * c)) / (2 * a)
    r2 = (-b - math.sqrt(b**2 - 4 * a * c)) / (2 * a)
    mins1 = get_closest(x0, x1, x2, r1)
    mins2 = get_closest(x0, x1, x2, r2)
    if (abs(mins1[0] - r1) + abs(mins1[1] - r1) <
        abs(mins2[0] - r2) + abs(mins2[1] - r2)):
        return mins1
    else:
        return mins2


"""
Metodo auxiliar de muller, para calculo de cercania de puntos
Entradas: solucion y 3 puntos adjacentes
Salidas: tupla de solucion y 2 puntos mas cercanos
"""
def get_closest(x0, x1, x2, r):
    closest = [x0, x1, r]
    if abs(x2 - r) < abs(x0 - r):
        closest[0] = x2
        if abs(x0 - r) < abs(x1 - r):
            closest[1] = x0
    elif abs(x2 - r) < abs(x1 - r):
        closest[1] = x2
    return closest

\end{lstlisting}

\section{Métodos Iterativos para Optimización}
\subsection{Método del Descenso Coordinado (optimizacion alternada)}
\subsubsection{Fórmula Matemática}
Regla de Gauss-Seidel (regla implementada):
\[x_{j}^{(k)} \in arg\; min\; f(x_{1}^{(k-1)}, ... ,x_{j-1}^{(k-1)}, x_{j},x_{j+1}^{(k-1)}, ... ,x_{n}^{(k-1)} )\]
\subsubsection{Descripción breve del método}
También conocido como optimización alternada, consiste en actualizar una de las variables, mientras la otras se mantienen fijas. Reduciendo la complejidad del problema a una optimización en una variable; utiliza la regla de Jacobi, Gauss-Seidel o aleatoria, para seleccionar cual variable actualizar en cada iteración.
\begin{itemize}
    \item Ventajas: la regla de Gauss-Seidel converge más rápido que la regla de Jacobi.
    \item Desventajas: la regla de Gauss-Seidel en algunas ocasiones no es convergente y cuando lo es frecuentemente es muy lento.
\end{itemize}
\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de Descenso Coordinado}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, variables, tolerancia mínima y función a evaluar\;}
    \Output{Aproximación de la solución y número de iteraciones}
    \SetAlgoLined

    \While{error \(\;\geq\;\)tolerancia}{
        \For{cada elemento de aproximacion}{
            evaluar demás elementos\;
            calcular derivada\;
            y resolver para mínimo\;
        }
        aumentar conteo de iteraciones\;
        recalcular error mediante la norma\;
     }
\end{algorithm}

\subsubsection{Código OCTAVE del método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método del Descenso Coordinado en Octave]
## Metodo iterativo del Descenso Coordinado para optimizacion en varias
## variables.
## Entradas: valores iniciales, variables de la funcion, tolerancia minima
## del resultado y funcion a evaluar en string.
## Salidas:Valor aproximado y numero de iteraciones
function [x0, iteration] = descenso_coordinado(x0, str_variables, tol, funcion)
    n = length(str_variables);
    f = str2func(funcion);
    error = tol;
    iteration = 0;
    for i = 1:n
        variables(i) = sym(str_variables(i));
    endfor
    
    function x0 = replace(x0, indice, variables, f)
        temp = num2cell(x0);
        temp(indice) = variables(indice);
        df = diff(f(temp{:}));
        soluciones = solve(df);
        m = length(soluciones);
        for i = 1:m
            temp(indice) = double(soluciones(i));
            if f(temp{:}) < f(num2cell(x0){:})
            x0 = cell2mat(temp);
            endif
        endfor
    endfunction

    while error >= tol
        prev = x0;
        for j = 1:n
            x0 = replace(x0, j, variables, f);
        endfor
        iteration += 1;
        error = norm(x0 - prev);
    endwhile
endfunction
\end{lstlisting}
\subsubsection{Código python del método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método del Descenso Coordinado en Python] 
from sympy import *
import math
import numpy as np

"""
Metodo iterativo del Descenso Coordinado para optimizacion
en varias variables.
Entradas: valores iniciales, variables de la funcion, tolerancia minima
del resultado y funcion a evaluar en string.
Salidas:Valor aproximado y numero de iteraciones
"""
def descenso_coordinado(x0, variables, tol, funcion):
    n = len(variables)
    iteracion = 0
    for i in range(n):
        variables[i] = Symbol(variables[i])
    funcion = sympify(funcion)
    f = lambdify(variables, funcion, "numpy")
    error = tol
    while error >= tol:
        prev = x0.copy()
        for j in range(n):
            x0 = replace(x0, j, variables[j], f)
        iteracion += 1
        difference = np.array(np.subtract(x0, prev), dtype=np.float64)
        error = np.linalg.norm(difference)

    return x0, iteracion

"""
Metodo auxiliar del Descenso Coordinado para calcular el minimo en una variable
y reemplazarlo
Entradas: valores iniciales, indice de la variable a optimizar,
variables de la funcion, funcion
Salidas: iteracion ya reemplazada
"""
def replace(x0, indice, variable, f):
    temp = x0.copy()
    temp[indice] = variable
    df = lambdify(variable, diff(f(*temp), variable), "numpy")
    soluciones = solve(df(variable))
    for solucion in soluciones:
        temp[indice] = solucion
        if(f(*temp) < f(*x0)):
            x0 = temp
    return x0
\end{lstlisting}


\subsection{Método del Gradiente Conjugado No Lineal}
\subsubsection{Fórmula Matemática}
\[x{(k+1)} = x^{(k)} + \alpha_{k} d^{(k)}\]
\vspace{1mm}
\[
       d^{(k+1)} = -g^{(k+1)} + \beta_{k}d^{(k)}\xrightarrow{}
       \text{con} \;\; d^{(0)} = -g^{(0)}
\]
\vspace{1mm}
\[g^{(k)} = \nabla{f(x^{(k)})}^t \]
\subsubsection{Descripción breve del método}
El método del gradiente conjugado no lineal, es un método que resuelve el problema de encontrar el mínimo de una función multivariable, de forma iterativa para una función continuamente diferenciable. Lo hace mediante una sucesión de de puntos a partir de un punto inicial, haciendo uso de diversos parámetros de actualización, y según reglas definidas.
\begin{itemize}
    \item Ventajas: Implementación sencilla, y en algunos casos converge rápidamente.
    \item Desventajas: como utiliza un número aleatorio en el cálculo de alpha, el comportamiento de aproximación varía con cada ejecución, dificultando su análisis y comparación.
\end{itemize}
\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método del Gradiente Conjugado}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Valores iniciales, variables, tolerancia mínima, función a evaluar y regla a usar}
    \Output{Aproximación de la solución y número de iteraciones}
    \SetAlgoLined
    \(g^{(0)} = \nabla{f(x^{(0)})}^t\)\;
    \(d^{(0)} = -g^{(0)}\)\;
    \While{error \(\;\geq\;\)tolerancia}{
        \(\alpha_{k} = 1\)\;
        \(\delta \in ]0,1[\)\;
        \While{\(f(x^{(k)} + \alpha_{k} d^{(k)}) - f(x^{(k)}) > \delta\alpha_k(g^{(k)})^t d^{(k)}\)}{
            \(\alpha_k = \alpha_k/2\)\;
        }
        \(x{(k+1)} = x^{(k)} + \alpha_{k} d^{(k)}\)\;
        \(g^{(k)} = \nabla{f(x^{(k)})}^t\)\;
        \(d^{(k+1)} = -g^{(k+1)} + \beta_{k}d^{(k)}\)\;
     }
\end{algorithm}

\subsubsection{Código OCTAVE del método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método del Gradiente Conjugado en Octave]
## Metodo iterativo del Gradiente Conjugado para optimizacion en varias
## variables.
## Entradas: valores iniciales, variables de la funcion, tolerancia minima
## del resultado, funcion a evaluar en string y regla a utilizar.
## Salidas:Valor aproximado y numero de iteraciones
function [x_k, iteration] = gradiente_conjugado(x0, str_variables, tol,
                                                funcion, regla_bk = 'FR')
    n = length(str_variables);
    f = str2func(funcion);
    derivatives = sym(zeros(1, n));
    for i = 1:n
        variables(i) = sym(str_variables(i));
    endfor
    
    for i = 1:n
        derivatives(i) = diff(f, variables(i));
        i += 1;
    endfor
    x_k = x0;
    error = tol;
    iteration = 0;
    
    function [grad] = gradiente(derivatives, variables, values, n)
        grad = zeros(1, n);
        
        for i = 1:n
            grad(i) = matlabFunction(derivatives(i))(num2cell(values){:});
            i += 1;
        endfor
    endfunction
    
    g_k_1 = g_k = gradiente(derivatives, variables, x_k, n);
    d_k = -1 * g_k;
    
    function [alpha] = get_alpha(f, variables, x_k, d_k, g_k)
        alpha = 1;
        delta = rand(1);
        f_arg = x_k + alpha * d_k;
        while double(f(num2cell(f_arg){:})) - double(f(num2cell(x_k){:})) > 
                    delta * alpha * g_k * transpose(d_k)
            alpha /= 2;
            f_arg = x_k + alpha * d_k;
        endwhile    
    endfunction
   
    function [beta] = get_beta(regla, g_k_1, g_k, d_k)
        if lower(regla) == 'cd'
            beta = norm(g_k_1)**2 / ((-1 * d_k)*transpose(g_k));
        elseif lower(regla) == 'dy'
            beta = norm(g_k_1)**2 / d_k*transpose(g_k_1 - g_k);
        else
            beta = norm(g_k_1)**2 / norm(g_k)**2;
        end    
    endfunction
    
    while error >= tol
        alpha = get_alpha(f, variables, x_k, d_k, g_k_1);
        x_k = x_k + alpha * d_k;
        g_k = g_k_1;
        g_k_1 = gradiente(derivatives, variables, x_k, n);
        d_k = -1 * g_k_1 + get_beta(regla_bk, g_k_1, g_k, d_k) * d_k;
        error = norm(g_k_1);
        iteration += 1;
    endwhile
endfunction
\end{lstlisting}
\subsubsection{Código python del método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método del Gradiente Conjugado en Python] 
from sympy import *
import numpy as np
from numpy import linalg
import random


"""
Metodo iterativo del Gradiente Conjugado para optimizacion en varias
variables.
Entradas: valores iniciales, variables de la funcion, tolerancia minima
del resultado, funcion a evaluar en string y regla a utilizar.
Salidas:Valor aproximado y numero de iteraciones
"""
def gradiente_conjugado(x0, variables, tol, funcion, regla_bk='FR'):
    variables = symbols(variables)
    funcion = sympify(funcion)
    f = lambdify(variables, funcion, "numpy")
    derivatives = []
    for variable in variables:
        derivatives.append(lambdify(variables, diff(funcion, variable)))
    x_k = np.array([x0])
    g_k_1 = g_k = gradiente(derivatives, x_k)
    d_k = -1 * g_k
    iteracion = 0
    error = tol
    while error >= tol:
        alpha = get_alpha(f, x_k, d_k, g_k_1)
        x_k = x_k + alpha * d_k
        g_k = g_k_1
        g_k_1 = gradiente(derivatives, x_k)
        d_k = -1 * g_k_1 + get_beta(regla_bk, g_k_1, g_k, d_k) * d_k
        error = linalg.norm(g_k_1)
        iteracion += 1
    return x_k[0].tolist(), iteracion


def gradiente(derivatives, values):
    result = []
    for derivative in derivatives:
        result.append(derivative(*values[0]))
    return np.array([result])


def get_alpha(f, x_k, d_k, g_k):
    alpha = 1
    delta = random.uniform(0, 1) #0.5 #random.randint(1, 9) * 0.1
    f_arg = x_k + alpha * d_k
    while f(*f_arg[0]) - f(*x_k[0]) > delta * alpha * g_k.dot(d_k.T):
        alpha /= 2
        f_arg = x_k + alpha * d_k
    return alpha


def get_beta(regla, g_k_1, g_k, d_k):
    if regla.lower() == 'cd':
        b = linalg.norm(g_k_1)**2 / ((-1 * d_k).dot(g_k.T))
    elif regla.lower() == 'dy':
        b = linalg.norm(g_k_1)**2 / d_k.dot((g_k_1 - g_k).T)
    else:
        b = linalg.norm(g_k_1)**2 / linalg.norm(g_k)**2
    return b

\end{lstlisting}


\section{Sistemas de Ecuaciones Lineales: Métodos Directos}
\subsection{Método de Eliminación Gaussiana}
\subsubsection{Fórmula Matemática}
Operaciones de fila (pivote):
    \[\xrightarrow{af_i + f_j}\]
Sustitución hacia atrás:
\vspace{1mm}
\[
       x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}x_{j})
\]

\subsubsection{Descripción breve del método}
El método de eliminación Gaussiana o eliminación de Gauss, es uno de los algoritmos más utilizado para resolver sistemas de ecuaciones. Consiste en transformar la matriz de coeficientes de un sistema lineal \(Ax=b\), en una matriz triangular superior realizando operaciones elementales de matrices sobre filas. La razón de hacer esto es que un sistema triangular es fácil y rápido de resolver en comparación a uno donde la matriz es densa.

Resolver el sistema triangular superior, consiste en un procedimiento rápido, denominado sustitución hacia atrás.

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de Eliminación Gaussiana}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Matriz de Coeficientes y matriz de términos independientes}
    \Output{Solución}
    \SetAlgoLined
    \For{elemento en matriz}{
        \If{\(i > j \;\&\&\; a(i,j) != 0\)}{
            // hacer operacion de fila\;
            \(multiplier = -1*a(i,j)/a(j,j)\)\;
                \For{k in 1:n}{a(i,k) = a(i,k) + a(j,k) * multiplier;} 
                        
            \(b(i) = b(i) + b(j) * multiplier\)\;
        }
        
        // sustitucion hacia atras\;
        \(x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}x_{j})\)
        
    }
    
\end{algorithm}

\subsubsection{Código OCTAVE del método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de Eliminación Gaussiana en Octave]
## Metodo directo de eliminacion_gaussiana para solucion de sistemas de 
## ecuaciones.
## Entradas: Matriz de Coeficientes y matriz de terminos independientes.
# Salidas: Solucion.
function [result] = eliminacion_gaussiana(a, b)
    n = length(a);
    function [a, b] =  triangular_superior(a, b, n)
        for j = 1:n
            for i = 1:n
                if i > j && a(i,j) != 0
                    multiplier = -1*a(i,j)/a(j,j);
                    for k = 1:n
                        a(i,k) = a(i,k) + a(j,k) * multiplier;
                    endfor
                    b(i) = b(i) + b(j) * multiplier;
                endif
            endfor
        endfor
    endfunction  
    
    function [result] = sustitucion_hacia_atras(a, b, n)
        result = zeros(1, n);
        i = n;
        while i > 0
            resultTemp = b(i);
            j = n;
            while j > 0
                if i == j
                    result(j) = resultTemp/a(i,j);
                    break
                else
                    resultTemp -= a(i,j)*result(j);
                end
                j -= 1;
            endwhile    
            i -= 1;
        endwhile
    endfunction
    
    [a, b] = triangular_superior(a, b, n);
    result = sustitucion_hacia_atras(a, b, n);
endfunction

\end{lstlisting}
\subsubsection{Código python del método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de Eliminación Gaussiana en Python] 
"""
Metodo directo de eliminacion gaussiana para solucion de sistemas de 
ecuaciones.
Entradas: Matriz de Coeficientes y matriz de terminos independientes.
Salidas: Solucion.
"""
def eliminacion_gaussiana(a, b):
    n = len(a)
    (a, b) = triangular_superior(a, b, n)
    result = sustitucion_hacia_atras(a, b, n)
    print(result)


def triangular_superior(a, b, n):
    for j in range(n):
        for i in range(n):
            if i > j and a[i][j] != 0:
                multiplier = -1*a[i][j]/a[j][j]
                for k in range(n):
                    a[i][k] = a[i][k] + a[j][k] * multiplier
                b[i] = b[i] + b[j]*multiplier
    return a, b


def sustitucion_hacia_atras(a, b, n):
    result = [None]*n
    i = n - 1
    while i >= 0:
        resultTemp = b[i]
        j = n - 1
        while j >= 0:
            if i == j:
                result[j] = resultTemp/a[i][j]
                break
            else:
                resultTemp -= a[i][j]*result[j]
            j -= 1
        i -= 1
    return result


\end{lstlisting}

\subsection{Método de Factorizacion LU}
\subsubsection{Fórmula Matemática}
Operaciones de fila (pivote):
    \[\xrightarrow{af_i + f_j}\]

Interesa el multiplicador \(a\)

Sustitución hacia atrás:
\vspace{1mm}
\[
       x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}x_{j})
\]

Sustitución hacia adelante:
\vspace{1mm}
\[
       x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=1}^{n}a_{ij}x_{j})
\]

\subsubsection{Descripción breve del método}
A la matriz A se le realiza la operación de restar y/o sumar el múltiplo
de otra fila convirtiendo en ceros las entradas debajo de la diagonal
principal, sin modificar los valores obtenidos en la diagonal
principal. Es decir, solo utilizar la operación de sumar y/o restar
el múltiplo de otra fila. Esta matriz resultante es la matriz U.

La matriz L se obtiene de los multiplicadores utilizados para convertir
en cero las entradas debajo de la diagonal principal. Para esto se
coloca los opuestos de los multiplicadores utilizados en las operaciones
de cada fila en la columna k, en la columna k de la matriz identidad
\(I_n\), debajo del primer elemento diagonal de \(I_n\) de forma ordenada.

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de Factorizacion LU}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Matriz de Coeficientes y matriz de términos independientes}
    \Output{Solución}
    \SetAlgoLined
    
    \For{elemento en matriz}{
        \If{\(i > j \;\&\&\; a(i,j) != 0\)}{
            // hacer operacion de fila\;
            \(multiplier = -1*a(i,j)/a(j,j)\)\;
             \(L(i, j) = -1 * multiplier\)\;
                \For{k in 1:n}{a(i,k) = a(i,k) + a(j,k) * multiplier;} 
                        
        }
        \If{i == j}{ L(i, j) = 1\;}
        
        // sustitucion hacia atras \(Ly = b\)\;
        \(y_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}y_{j})\)\;
        // sustitucion hacia adelante \(Ux = y\)\;
        \(x_i = \frac{1}{a_{ii}}(y_i-\sum_{1}^{n}a_{ij}x_{j})\)\;
        
    }
    
\end{algorithm}

\subsubsection{Código OCTAVE del método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de Factorizacion LU en Octave]
## Metodo directo de factorizacion LU para solucion de sistemas de 
## ecuaciones.
## Entradas: Matriz de Coeficientes y matriz de terminos independientes.
# Salidas: Solucion.
function [result] = factorizacion_LU(a, b)
    n = length(a);
    
    function [L, a] =  get_LU(a, n)
        L = zeros(n);
        for j = 1:n
            for i = 1:n
                if i > j && a(i, j) != 0
                    multiplier = -1 * a(i, j) / a(j, j);
                    L(i, j) = -1 * multiplier;
                    for k = 1:n
                        a(i, k) = a(i, k) + a(j, k) * multiplier;
                    endfor
                elseif i == j
                    L(i, j) = 1;
                end
            endfor
        endfor
    endfunction 
    
    function [result] = sustitucion_hacia_atras(a, b, n)
        result = zeros(1, n);
        i = n;
        while i > 0
            resultTemp = b(i);
            j = n;
            while j > 0
                if i == j
                    result(j) = resultTemp/a(i,j);
                    break
                else
                    resultTemp -= a(i,j)*result(j);
                end
                j -= 1;
            endwhile    
            i -= 1;
        endwhile
    endfunction
    
    function [result] = sustitucion_hacia_adelante(a, b, n)
        result = zeros(1, n);
        for i = 1:n
            resultTemp = b(i);
            for j = 1:n
                if i == j
                    result(j) = resultTemp/a(i,j);
                    break
                else
                    resultTemp -= a(i,j)*result(j);
                end
            endfor
        endfor    
    endfunction
    
    [L, U] = get_LU(a, n);
    y = sustitucion_hacia_adelante(L, b, n);
    result = sustitucion_hacia_atras(U, y, n);
endfunction

\end{lstlisting}
\subsubsection{Código python del método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de Factorizacion LU en Python] 

"""
Metodo directo de factorizacion LU para solucion de sistemas de 
ecuaciones.
Entradas: Matriz de Coeficientes y matriz de terminos independientes.
Salidas: Solucion.
"""
def factorizacion_LU(a, b):
    n = len(a)
    L, U = get_LU(a, n)
    y = sustitucion_hacia_adelante(L, b, n)
    return sustitucion_hacia_atras(U, y, n)


def get_LU(a, n):
    L = [[0 for i in range(n)] for j in range(n)]
    for j in range(n):
        for i in range(n):
            if i > j and a[i][j] != 0:
                multiplier = -1 * a[i][j] / a[j][j]
                L[i][j] = -1 * multiplier
                for k in range(n):
                    a[i][k] = a[i][k] + a[j][k] * multiplier
            elif i == j:
                L[i][j] = 1
    return L, a


def sustitucion_hacia_atras(a, b, n):
    result = [None]*n
    i = n - 1
    while i >= 0:
        resultTemp = b[i]
        j = n - 1
        while j >= 0:
            if i == j:
                result[j] = resultTemp/a[i][j]
                break
            else:
                resultTemp -= a[i][j]*result[j]
            j -= 1
        i -= 1
    return result


def sustitucion_hacia_adelante(a, b, n):
    result = [None]*n
    for i in range (n):
        resultTemp = b[i]
        for j in range(n):
            if i == j:
                result[j] = resultTemp/a[i][j]
                break
            else:
                resultTemp -= a[i][j]*result[j]
    return result


\end{lstlisting}

\subsection{Método de Factorizacion de Cholesky}
\subsubsection{Fórmula Matemática}
\[l_{i,j} = \begin{cases} 
      \sqrt{a_{i,j}-\sum_{k=1}^{j-1} l_{j,k}^2} & \text{si } i = j \\
      \frac{1}{l_{j,j}}(a_{i,j} - \sum_{k=1}^{j-1} l_{i,k}l_{j,k}) & \text{si } i > j 
   \end{cases}
\]

Sustitución hacia atrás:
\vspace{1mm}
\[
       x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}x_{j})
\]

Sustitución hacia adelante:
\vspace{1mm}
\[
       x_i = \frac{1}{a_{ii}}(b_i-\sum_{j=1}^{n}a_{ij}x_{j})
\]

\subsubsection{Descripción breve del método}
La factorización o descomposición de Cholesky es un tipo de
factorización para una matriz simétrica definida positiva, la cual
puede ser descompuesta como el producto de una matriz triangular
inferior y la traspuesta de la matriz triangular inferior.

La matriz triangular inferior es el triángulo de Cholesky de la matriz
original.

\subsubsection{Pseudocódigo del método}
\begin{algorithm}[H]
\caption{Método de Factorizacion de Cholesky}
    \SetKwInOut{Input}{entradas}\SetKwInOut{Output}{salidas}
    \Input{Matriz de Coeficientes y matriz de términos independientes}
    \Output{Solución}
    \SetAlgoLined
    
    
    \For{elemento en matriz}{
        \If{\(i==j\)}{
            sum1 = 0\;
            \For{k = 1:j}{\(sum1 += L(j, k)**2\)\;}
            \(L(i, j) = sqrt(a(i, j) - sum1)\)\;
        }
        \If{\(i > j\)}{
            sum2 = 0\;
            \For{k = 1:j}{
            \(sum2 += L(i, k)*L(j, k)\)\;}
        }
         \(L(i, j) = (a(i, j) - sum2) / L(j, j)\)\;
        // sustitucion hacia atras \(Ly = b\)\;
        \(y_i = \frac{1}{a_{ii}}(b_i-\sum_{j=i+1}^{n}a_{ij}y_{j})\)\;
        // sustitucion hacia adelante \(L^tx = y\)\;
        \(x_i = \frac{1}{a_{ii}}(y_i-\sum_{1}^{n}a_{ij}x_{j})\)\;
        
    }
    
\end{algorithm}

\subsubsection{Código OCTAVE del método}
\begin{lstlisting}[language=OCTAVE, caption=Implementación del método de Factorizacion de Cholesky en Octave]

## Metodo directo de factorizacion de Cholesky para solucion de sistemas de 
## ecuaciones.
## Entradas: Matriz de Coeficientes y matriz de terminos independientes.
## Salidas: Solucion.
function [result] = cholesky(a, b)
    n = length(a);
    
    function [L] = get_L(a, n)
        L = zeros(n);
        for i = 1:n
            for j = 1:n
                if i == j
                    sum1 = 0;
                    for k = 1:j
                        sum1 += L(j, k)**2;
                    endfor
                    L(i, j) = sqrt(a(i, j) - sum1);
                else 
                    if i > j
                        sum2 = 0;
                        for k = 1:j
                            sum2 += L(i, k)*L(j, k);
                        endfor    
                        L(i, j) = (a(i, j) - sum2) / L(j, j);
                    end
                end
            endfor
        endfor    
    endfunction

    function [result] = sustitucion_hacia_atras(a, b, n)
        result = zeros(1, n);
        i = n;
        while i > 0
            resultTemp = b(i);
            j = n;
            while j > 0
                if i == j
                    result(j) = resultTemp/a(i,j);
                    break
                else
                    resultTemp -= a(i,j)*result(j);
                end
                j -= 1;
            endwhile    
            i -= 1;
        endwhile
    endfunction
    
    function [result] = sustitucion_hacia_adelante(a, b, n)
        result = zeros(1, n);
        for i = 1:n
            resultTemp = b(i);
            for j = 1:n
                if i == j
                    result(j) = resultTemp/a(i,j);
                    break
                else
                    resultTemp -= a(i,j)*result(j);
                end
            endfor
        endfor    
    endfunction
    L = get_L(a, n);
    y = sustitucion_hacia_adelante(L, b, n);
    result = sustitucion_hacia_atras(transpose(L), y, n);
endfunction    

\end{lstlisting}
\subsubsection{Código python del método}
%Python code highlighting
\begin{lstlisting}[language=Python, caption=Implementación del método de Factorizacion de Cholesky en Python] 

from math import *

"""
Metodo directo de factorizacion de Cholesky para solucion de sistemas de 
ecuaciones.
Entradas: Matriz de Coeficientes y matriz de terminos independientes.
Salidas: Solucion.
"""
def cholesky(a, b):
    n = len(a)
    L = get_L(a, n)
    Lt =[[L[j][i] for j in range(n)] for i in range(n)]
    y = sustitucion_hacia_adelante(L, b, n)
    return sustitucion_hacia_atras(Lt, y, n)

def get_L(a, n):
    L = [[0 for i in range(n)] for j in range(n)]

    for i in range(n):
        for j in range(n):
            if i == j:
                sum1 = 0
                for k in range(j):
                    sum1 += L[j][k]**2
                L[i][j] = sqrt(a[i][j] - sum1)
            elif i > j:
                sum2 = 0
                for k in range(j):
                    sum2 += L[i][k]*L[j][k]
                L[i][j] = (a[i][j] - sum2) / L[j][j]
    return L

def sustitucion_hacia_atras(a, b, n):
    result = [None]*n
    i = n - 1
    while i >= 0:
        resultTemp = b[i]
        j = n - 1
        while j >= 0:
            if i == j:
                result[j] = resultTemp/a[i][j]
                break
            else:
                resultTemp -= a[i][j]*result[j]
            j -= 1
        i -= 1
    return result


def sustitucion_hacia_adelante(a, b, n):
    result = [None]*n
    for i in range (n):
        resultTemp = b[i]
        for j in range(n):
            if i == j:
                result[j] = resultTemp/a[i][j]
                break
            else:
                resultTemp -= a[i][j]*result[j]
    return result
    
\end{lstlisting}



\iffalse
\section{Sistemas de Ecuaciones Lineales: Métodos Iterativos}

\section{Interpolación}

\section{Regresión Numérica}

\section{Diferenciación Numérica}

\section{Integración Numérica}

\section{Ecuaciones Diferenciales Numéricas}

\section{Método Iterativos para Calcular Valores Propios}

\fi
\end{document}
